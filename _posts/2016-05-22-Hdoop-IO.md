---
layout: post
title: Hadoop IO
date: 2015-05-22
excerpt: "About Integrity, Compression, Serialization, Data Structure."
bigDate: true
comments: true
tag: [Hadoop, Integrity, Compression, Serializtion, Data Structrue]
---

# 1. 数据的一致性

> 什么是数据一致性  
> 一致性的实现方式  
> HDFS 上的数据一致性  
> 本地文件系统  
> Checksum 文件系统


## 1.1 什么是数据一致性

由于数据在网络或者硬盘间传输时会不可避免发生错误。所以需要对传输前后的数据进行比对，保证数据传输前后的**一致性**。

## 1.2 如何实现

当数据进入系统时计算一个验证值（[参考 wiki 的解释 checksum](https://en.wikipedia.org/wiki/Checksum)）。当数据到达另一个地方时，再用同样的方法计算一遍验证值，比对前后的验证值是否一致来确认数据的正确性。常用的验证编码是：CRC-32。但是这种方法只能用于验证，不能用于纠错。

## 1.3 一致性在HDFS上的实现

* Datanodes 负责在***所有***数据存储数据前进行验证。如果 Datanodes 发现有问题，会向用户端抛出异常，由客户端处理。

* 单个区块验证和后台周期性全部区块验证。
* HDFS 的纠错机制：checksum ＋ 利用其副本机制 ＝ 纠错 （简单来讲就是当发现有错误文件时，通过其他 datanode 上存储的副本进行替换。）

## 1.4 本地文件系统（需要进一步理解）

本地文件系统是 client-side，当你写出一个文件时，文件系统会自动生成一个 .crc 格式的 checksum 与文件一起保存。

---

# 2. 压缩

数据的压缩：减少存储空间。增加传输速度。如何在 hadoop 使用压缩格式／工具／算法很重要。

## 编码

Codec－解压缩技术的算法实现。hadoop 中由 CompressCodes 接口实现。

## 压缩和输入*间隙*

看一个压缩数据是否可以由 mapreduce 处理，必须理解压缩方式是否支持 splitting。间隙是用来将数据划分为区块，以便 map 任务可以插入进去对不同区块进行独立的处理。

## 在 mapreduce 中使用压缩

对 Configuration 设置 *mapreduce.output.fileoutputformat.compress* 属性为 true，设置 *mapre duce.output.fileoutputformat.compress.codec* 的属性为你需要使用的压缩 Codec 所属的类。  
下例是对输出开启 gzip 压缩：

~~~java
Configuration conf = new Configuration();
conf.setBoolean(Job.MAP_OUTPUT_COMPRESS, true);
conf.setClass(Job.MAP_OUTPUT_COMPRESS_CODEC, GzipCodec.class, CompressionCodec.class); 
Job job = new Job(conf);
~~~

# 3. 序列化
> Writable 接口  
> Writable 类  
> 定制你的 Writable 类  
> 序列化框架

## 3.1 Writable 接口

该接口定义了两个方法，一个用于将它的状态写出到二进制输出流（DataOutputStream），一个用于从二进制输如流（DataInputStream）中读取数据。

~~~java
public interface Writable {	void write(DataOutput out) throws IOException;
	void readFields(DataInput in) throws IOException;
}
~~~

两个例子如下：  
序列化：

~~~java
IntWritable writable = new IntWritable(); 
writable.set(163);
//或者：IntWritable writable = new IntWritable(163);

public static byte[] serialize(Writable writable) throws IOException { 
	ByteArrayOutputStream out = new ByteArrayOutputStream(); 
	DataOutputStream dataOut = new DataOutputStream(out);
	writable.write(dataOut);
	dataOut.close();
	return out.toByteArray(); 
}

byte[] bytes = serialize(writable); 
assertThat(bytes.length, is(4));
~~~

去序列化，bytes 变量承上：

~~~java
public static byte[] deserialize(Writable writable, byte[] bytes) throws IOException {
	ByteArrayInputStream in = new ByteArrayInputStream(bytes); 
	DataInputStream dataIn = new DataInputStream(in);
	writable.readFields(dataIn);	dataIn.close();	return bytes; 
}

IntWritable newWritable = new IntWritable(); 
deserialize(newWritable, bytes);
assertThat(newWritable.get(), is(163));
~~~

***WritableComparable and Rawcomparators***

这两个接口的优势在于不需要将序列化的数据**去序列化（将 Stream 变回为 object）**而可以进行比较。  
Comparator: 

~~~java
public interface RawComparator<T> extends Comparator<T> {	public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2); 
}
~~~

首先通过以下方式获得一个 comparator：

~~~java
RawComparator<IntWritable> comparator =            WritableComparator.get(IntWritable.class);
            
~~~
comparator 既可以用于比较两个 IntWritable objects:

~~~javaIntWritable w1 = new IntWritable(163);IntWritable w2 = new IntWritable(67); 
assertThat(comparator.compare(w1, w2), greaterThan(0));
~~~

也可以比较两个序列化的代表：

~~~java
byte[] b1 = serialize(w1);byte[] b2 = serialize(w2);assertThat(comparator.compare(b1, 0, b1.length, b2, 0, b2.length), greaterThan(0));
~~~

## 3.2 Writable class

<figure>
	<a href="http://breakdimbo.github.io/images/Hadoop-IO-Writable-class.png"><img src="http://breakdimbo.github.io/images/Hadoop-IO-Writable-class.png"></a>
</figure>


### Text calss

等价于 Java 中的 String。最大值2G。

### ObjectWritable and GenericWritable

类似于容器，用来 Wrapper Java primitives, String, enum, Writable, null, or arrays of any of these types。

### Writable collections


## 3.3 定制你的 Writable 类

建议使用 serialization framework, like Avro。

例子：

~~~java
public class TextPair implements WritableComparable<TextPair> {
	private Text first; 
	private Text second;
		public TextPair() {		set(new Text(), new Text());	}
		public TextPair(String first, String second) { 
		set(new Text(first), new Text(second));	}
		public TextPair(Text first, Text second) { 
		set(first, second);	}	public void set(Text first, Text second) {
		this.first = first;		this.second = second; 
	}
		public Text getFirst() { 
		return first;	}
		public Text getSecond() { 
		return second;	}
		@Override	public void write(DataOutput out) throws IOException { 
		first.write(out);		second.write(out);	}
		@Override	public void readFields(DataInput in) throws IOException { 
		first.readFields(in);		second.readFields(in);	}
		@Override	public int hashCode() {		return first.hashCode() * 163 + second.hashCode();	}
		@Override	public boolean equals(Object o) { 
		if (o instanceof TextPair) {      		TextPair tp = (TextPair) o;			return first.equals(tp.first) && second.equals(tp.second); 
		}		return false; 
	}
		@Override	public String toString() { 
		return first + "\t" + second;	}
		@Override	public int compareTo(TextPair tp) { 
		int cmp = first.compareTo(tp.first); 
		if(cmp!=0){			return cmp; 
		}		return second.compareTo(tp.second); 
	}}

~~~

如何使用定制化的 Comparators?

## 3.4 序列化框架

